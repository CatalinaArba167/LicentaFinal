import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import BaggingRegressor

data=pd.read_csv("C:/Users/arbac/Downloads/LicentaAIModel/car_price_prediction.csv")

# Check for duplicate rows in the DataFrame
duplicate_rows = data[data.duplicated()]

# Remove duplicate rows
data = data.drop_duplicates()


# Assuming `data` is your DataFrame

# Transform the name of the columns to snake case
data.columns = data.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')
#----------------------------------------------------------------------------
# ENGINE VOLUME

# Ensure all values in 'engine_volume' are strings
data['engine_volume'] = data['engine_volume'].astype(str)

# Create a new binary column 'is_turbo'
data['is_turbo'] = data['engine_volume'].apply(lambda x: 1 if 'Turbo' in x else 0)

# Extract the numeric part of 'engine_volume'
data['engine_volume'] = data['engine_volume'].str.replace(' Turbo', '').astype(float)

# Handle NaN and infinity values
data.replace([np.inf, -np.inf], np.nan, inplace=True)
data.dropna(inplace=True)

#----------------------------------------------------------------------------
# MILEAGE

# Remove ' km' from 'mileage' and convert to int
data['mileage'] = data['mileage'].str.replace(' km', '').astype(int)

#----------------------------------------------------------------------------
# LEATHER INTERIOR

# Map 'leather_interior' to binary values
data['leather_interior'] = data['leather_interior'].map({'Yes': 1, 'No': 0})

# Drop the 'id' and 'levy' columns
data.drop(columns=['id', 'levy'], axis=2)



# Ensure all values in 'prod_year' and 'mileage' are positive
# data['prod._year'] = data['prod._year'].apply(lambda x: x if x > 0 else 1)
# data['mileage'] = data['mileage'].apply(lambda x: x if x > 0 else 1)
data = data[(data['prod._year'] > 0) & (data['mileage'] > 0)]

data['prod._year']=np.log(data['prod._year']+1)
data['mileage']=np.log(data['mileage']+1)
# data['levy']=np.log(data['levy']+1)




categorical_columns = ['manufacturer', 'model', 'category', 'fuel_type', 'gear_box_type', 'drive_wheels','doors','wheel','color']

# Apply one-hot encoding to categorical columns
data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)





# Assuming 'price' is the target variable and all other columns are features
x = data.drop(['price'], axis=1)
y = data['price']

# Replace '-' with NaN
x.replace('-', np.nan, inplace=True)

# Convert all columns to numeric, forcing non-convertible values to NaN
x = x.apply(pd.to_numeric, errors='coerce')

# Check for and handle NaN or infinite values
x.replace([np.inf, -np.inf], np.nan, inplace=True)

# Drop rows where either x or y have NaN values
x = x.dropna()
y = y.loc[x.index]

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Optionally, scale the data
scaler = StandardScaler()
x_train_s = scaler.fit_transform(x_train)
x_test_s = scaler.transform(x_test)

# Fit the linear regression model
reg = LinearRegression()
reg.fit(x_train_s, y_train)

# Evaluate the model
y_pred = reg.predict(x_test)
score = reg.score(x_test, y_test)

print(f"Model score: {score}")

#----------------------------------------------------------------------------
# RANDOM FOREST

forest=RandomForestRegressor()
forest.fit(x_train, y_train)

forest.score(x_test,y_test)

#------------------------------------------------------------------------------
#BAGGING


bagging=BaggingRegressor()
bagging.fit(x_train, y_train)

bagging.score(x_test,y_test)
